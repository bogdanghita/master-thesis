\section{Compression learning process}


% ----------------------- paths to graphics ------------------------



% ----------------------- contents from here ------------------------
% 

We define \textit{compression learning} as the process of learning the best representation of a dataset in terms of storage size. This section describes: 1) the general optimization problem of learning the best compression tree  based on a sample of data; 2) an exhaustive learning algorithm and its cost model; 3) a greedy algorithm and the heuristics it uses for decision making; 4) an architecture for combining multiple learning algorithms.

\input{5_automatic_learning/learning_process/optimization_problem}

\input{5_automatic_learning/learning_process/compression_estimators}

\input{5_automatic_learning/learning_process/recursive_exhaustive}

\input{5_automatic_learning/learning_process/pattern_selection}

\input{5_automatic_learning/learning_process/iterative_greedy}

% \subsection{Recursive greedy learning}
% \label{sub:learning:recursivegreedy}

\iffalse
TODO-1-1: result: worse than iterative learning since it misses multi-column opportunities
TODO-1-2: advantage: faster execution (one column at a time; suitable for columnar storage)
TODO-2: comparison with the other algorithms & improvements brought to OR by them
\fi

\input{5_automatic_learning/learning_process/multistage}

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
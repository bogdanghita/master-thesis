\subsection{Optimization problem}
\label{subsec:learningprocess:optimizationproblem}


% ----------------------- paths to graphics ------------------------



% ----------------------- contents from here ------------------------
% 

We can define the learning process as follows: given a sample from a dataset, its schema and a set of pattern detectors as input, output a compression tree that, when applied to the dataset, produces a compressed representation of it of minimum disk size.

The schema is a list of columns and their data types. The pattern detectors are implementations of the \nameref{subsec:genericpd}---receives the columns as input, evaluates the sample and returns a list of \textit{(expression node, evaluation result)} tuples. Adding an \textit{expression node} to the compression tree means: 1) altering the schema by deleting existing columns and creating new ones; 2) altering the sample by applying the \textit{compression operator} to the input columns and generating new data. The learning process can go on by recursively feeding the new schema and sample data to the pattern detectors, resulting in new \textit{(expression node, evaluation result)} tuples. This recursive process stops when no pattern detector outputs any result anymore---no pattern matches on the current schema and data.

Each decision of adding or not adding an \textit{expression node} to the compression tree generates a new solution. This leads to a total number of \(2^n\) possible solutions (different compression trees), where \(n\) is the total number of \textit{expression nodes} generated by the recursive process (\(n\) binary decisions: \(1\) means adding a node and \(0\) not adding it). The total number of \textit{expression nodes} (\(n\)) depends on how well pattern detectors match on the initial columns and the newly generated ones. This is entirely dependent on the characteristics of the data and the patterns that were evaluated on it. In the worst case, all pattern detectors will match on any column, leading to the following expression for \(n\):
\begin{equation}
\label{eq:optimizationproblem:n}
    n = c_{in} \times (p \times \mathit{avg}(n_{p}) \times b) ^ h
\end{equation}
\begin{equation}
\label{eq:optimizationproblem:b}
    b = \mathit{avg}(c_{out})
\end{equation}
where:
\begin{itemize}
    \item[] \(n\) = total number of expression nodes
    \item[] \(c_{in}\) = number of input columns
    \item[] \(p\) = number of pattern detectors
    \item[] \(n_{p}\) = number of expression nodes returned by a pattern detector
    \item[] \(b\) = branching factor of the compression tree
    \item[] \(h\) = height of the compression tree
    \item[] \(c_{out}\) = number of output columns of an expression node
\end{itemize}

The score of each solution is given by the size of the compressed data that resulted after applying the compression tree to the dataset. The goal of the learning process is to choose the one that gives the smallest size.

The computational effort needed for each individual \textit{expression node} consists of: 1) applying the compression operator on its input columns to generate the new data (feeding each tuple in the sample data to the operator); 2) evaluating the pattern detectors on all the new columns (feeding each tuple in the sample data to each pattern detector). Moreover, some pattern detectors may need to evaluate combinations of columns instead of individual columns, which requires all the existing columns to be reevaluated for every newly generated column (e.g. \nameref{subsec:pd:columncorrelation} evaluates all pairs of 2 columns to determine the correlation coefficient between them).

\iffalse
TODO:
better formalize problem
https://en.wikipedia.org/wiki/Optimization\_problem
\fi

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
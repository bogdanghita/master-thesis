\subsection{Recursive exhaustive learning}
\label{sub:learning:recursiveexhaustive}


% ----------------------- paths to graphics ------------------------



% ----------------------- contents from here ------------------------
% 

This section presents an exhaustive recursive algorithm for compression learning. It uses the compression estimators (\(E\)) as a cost model (\ref{sub:estimators}~\nameref{sub:estimators}) and the pattern detectors (\(P\)) defined in \ref{sec:pd}~\nameref{sec:pd}. The algorithm takes as input a column (\(c_{in}\)) and outputs the best compression tree (\(T_{out}\)) with respect to the compression estimators: the one that produces the smallest representation of the column when used to compress it. The algorithm is recursive and takes a depth-first approach. For this reason, it is not suitable for pattern detectors that work with more than one column (e.g. \nameref{subsec:pd:columncorrelation}).

In addition to the input column \(c_{in}\), the recursive function receives a compression tree \(T_{in}\) (initially empty) and an optional max height parameter \(h_{max}\) (the maximum height of the compression tree). The compression tree \(T_{in}\) is the partial solution built in the recursive process so far. The role of the \(h_{max}\) parameter is to limit the dimension of the problem and avoid scenarios where the algorithm does not finish .

The algorithm tries all possibilities to compress the input column \(c_{in}\): 1) with leaf compression nodes (\(N_{leaf}\), e.g. RLE, FOR, DICT); 2) with internal (non-leaf) expression nodes (\(N_{internal}\), e.g. \nameref{subsec:pd:numericstrings}, \nameref{subsec:pd:charsetsplit}, etc.); 3) without compression. For each possibility it estimates the size of the resulting columns (\(c_{out}\)) in the following way: 1) for leaf expression nodes (\(N_{leaf}\)) and no compression, the estimators \(E\) directly provide the size; 2) for non-leaf expression nodes (\(N_{internal}\)), the size is computed by recursively applying the same algorithm on the output columns \(c_{out}\) of the expression node \(N_{internal}\) and adding the resulted sizes together. The recursive call returns a new compression tree \(T_{c}\) for each output column of \(N_{internal}\). A special case is the DICT expression node, which will have 2 estimated sizes: 1) direct estimation from the \nameref{subsub:estimator:dict} \(E_{dict}\) as a leaf node; 2) recursive call estimation (other pattern detectors \(P\) can be applied on its output column, e.g. \nameref{subsec:pd:columncorrelation}).

Out of all the possibilities to compress \(c_{in}\), the one which gives the smallest size is chosen and \(T_{in}\) is updated with either the leaf compression node \(N_{leaf}\) or with the set of compression trees \(T_{c}\) resulted from the recursive call. The algorithm returns the updated compression tree \(T_{out}\).

The termination conditions of the recursive algorithm are: 1) all possibilities to compress \(c_{in}\) are leaf compression nodes (i.e. there is no \(N_{internal}\) to generate new output columns \(c_{out}\)); 2) the max height of the compression tree \(h_{max}\) is reached. Termination condition 1) occurs when no pattern detector \(P\) outputs any compression nodes \(N_{internal}\). A pattern detector \(P\) does not output any compression node \(N_{internal}\) when: 1) \(c_{in}\) is not compatible with the pattern (e.g. \nameref{subsec:pd:numericstrings} pattern detector only applies to \verb|VARCHAR| columns; see \textit{select\_column} in \ref{sec:pd}~\nameref{sec:pd}); 2) the data in \(c_{in}\) does not match the pattern (e.g. a column with multiple values does not match the \nameref{subsec:pd:constant} pattern).

The algorithm is described in listings \ref{lst:learning:recursiveexhaustive:build_T} and \ref{lst:learning:recursiveexhaustive:apply_N}.

\begin{lstlisting}[language=Python,
label={lst:learning:recursiveexhaustive:naming},
caption={Naming conventions}]
c = column
T = compression tree
E = compression estimator
P = pattern detector
N = compression node
\end{lstlisting}

\begin{lstlisting}[language=Python,
label={lst:learning:recursiveexhaustive:build_T},
caption={build\_T (recursive exhaustive)}]
def build_T(c_in, T_in, P_list, E_list):
  sol_list = list()
  # estimator evaluation
  for E in E_list:
    size = E.evaluate(c_in)
    sol_list.append((size, T_in))
  # pattern detection 
  N_list = list()
  for P in P_list:
    N_p_list = P.evaluate(c_in)
    N_list.extend(N_p_list)
  # recursive evaluation
  for N in N_list:
    (size, T_out) = apply_N(c_in, N, T_in, P_list, E_list)
    sol_list.append((size, T_out))
  # return best solution
  return min(sol_list, key=size)
\end{lstlisting}

\begin{lstlisting}[language=Python,
label={lst:learning:recursiveexhaustive:apply_N},
caption={apply\_N (recursive exhaustive)}]
def apply_N(c_in, N, T_in, P_list, E_list):
  T_out = copy(T_in)
  T_out.update(N)
  # recursive call for all output columns
  sol_list = list()
  for c_out in N.c_out_list:
    (size_c, T_c) = build_T(c_out, T_out, P_list, E_list)
    sol_list.append((size_c, T_c))
  # merge results
  size_out = 0
  for (size_c, T_c) in sol_list:
    size_out += size_c
    T_out = merge(T_out, T_c)
  # return merged result
  return (size_out, T_out)
\end{lstlisting}
\bigskip

The complexity of the algorithm can be described as:
\begin{equation}
\label{eq:learning:recursiveexhaustive:single}
    (p \times \mathit{avg}(n_{p}) \times b) ^ {h_{max}}
\end{equation}
where:
\begin{itemize}
    \item[] \(p\) = number of pattern detectors
    \item[] \(n_{p}\) = number of expression nodes returned by a pattern detector
    \item[] \(b\) = branching factor of the compression tree (as defined in Equation \ref{eq:optimizationproblem:b})
    \item[] \(h_{max}\) = maximum height of the compression tree
\end{itemize}
The height is bounded by the \(h_{max}\) parameter. Despite the high complexity, the size of the problem remains relatively small due to the high selectivity of the pattern detectors \(P\) (see \textit{select\_column} in \ref{sec:pd}~\nameref{sec:pd}). This is the complexity of learning the compression tree for a single column, while for multiple columns the complexity becomes:
\begin{equation}
\label{eq:learning:recursiveexhaustive:multi}
    c_{in} \times (p \times \mathit{avg}(n_{p}) \times b) ^ {h_{max}}
\end{equation}
where:
\begin{itemize}
    \item[] \(c_{in}\) = number of input columns
\end{itemize}

This complexity is significantly better than the complexity of the general optimization problem described in \ref{subsec:learningprocess:optimizationproblem} (\(2^n\), where \(n\) is the total number of expression nodes generated in the learning process, as described in Equation~\ref{eq:optimizationproblem:n}). The reason for this improvement is the single-column approach---not considering pattern detectors \(P\) that combine multiple columns. The solutions of a column \(c_{i}\) do not depend on the solutions of other columns \(c_{j}\) that are not on the path from \(c_{i}\) to its root input column \(c_{r}\). This is because a choice made for \(c_{j}\) does not influence the choices that can be made for \(c_{i}\).
This property allows a depth-first exploration of the solutions, significantly reducing the complexity.

However, there is a cost for not considering multi-column pattern detectors: the algorithm misses compression opportunities (e.g. one column represented as a function of another through \nameref{subsec:pd:columncorrelation}). It is exhaustive and yet cannot produce the best result. Section \ref{sub:learning:iterative}~\nameref{sub:learning:iterative} presents a greedy approach that considers multi-column patter detectors. Moreover, the \nameref{sub:learning:multistage} in section \ref{sub:learning:multistage} addresses this issue by chaining together multiple learning algorithms.

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Conclusion and future work} % top level followed by section, subsection


% ----------------------- paths to graphics ------------------------

% change according to folder and file names


% ----------------------- contents from here ------------------------
% 

\section{Conclusion}

In this thesis we explored the concept of \textit{whitebox compression}. We analysed the Public BI benchmark in search for compression opportunities. Based on our findings we defined a new compression model which uses elementary operators to represent data more compactly. We further defined an automatic compression learning process and created a proof-of-concept implementation to measure its feasibility and compression potential. Let us recall the research questions together with their answers.

\textbf{What does real user generated data look like---specifically in the case of the Public BI benchmark?}
From our analysis we concluded that real data is redundant and represented in inefficient ways, from "suboptimal" datatypes, to highly correlated or even duplicate columns. Most of the string columns have a high repetition factor, making them suitable for dictionary-like encoding techniques.
\iffalse
% \item What patterns can we find in the columns of each dataset?
% \item Inefficient ways of representing data?
% \item "Wrong" type used to define data (e.g. number stored as string, etc.)?
% - Public BI benchmark analysis---an analysis of real user-generated data from the perspective of compression
\fi

\textbf{How good are existing compression schemes at compressing real data?} Due to the high number of numeric columns and the low number of unique values in string columns---present in the Public BI benchmark---the data is already suitable for compression with existing lightweight methods. VectorWise achieves an overall compression ratio of 2.58 on the benchmark. However, real data has a considerable compression potential that is not exploited by these systems.
\iffalse
% \item Do they make the most out of the properties of the data?
% \item Is there room for improvement?
% - vectorwise \& estimator results
\fi

\textbf{Can we represent the logical columns more compactly through an expression tree composed of elementary operators?} Based on the compression opportunities found in the datasets we defined an expression language that enables more efficient representation of the data. The \textit{whitebox compression} model achieves high compression ratios by splitting columns into subcolumns, storing data in the appropriate format, and representing columns as functions of other columns. This complex compression schemes are actually the result of simple recursive representation of columns through elementary operators. Combined with a transparent mechanism of handling (and recursively compressing) exceptions, we manage to store data more compactly.
\iffalse
% \item What kind of operators are suitable for expressing the data and transforming it to physical columns?
% \item What will the compression ratio be?
% \item Can we exploit correlations between multiple logical columns by sharing the physical columns in their expressions?
% \textit{whitebox compression}---a new generic, extensible, recursive and transparent compression model for columnar databases, which achieves higher compression ratios by representing data more compactly through elementary operator expressions and creates opportunities for faster query execution
\fi

\textbf{Can we create an automatic learning process that will generate suitable compression trees for each column?}
We defined and implemented a set of pattern detectors which identify compression opportunities present in the data and evaluate its potential for \textit{whitebox} representation. We further defined the optimization problem of finding the best representation for a set of columns and proposed algorithms that solve it using an estimator cost model and greedy heuristics. We validated our compression model with a proof-of-concept implementation, achieving an overall compression ratio of 3.58 on the full data and 5.16 on the columns represented through \textit{whitebox compression}.\\
\iffalse
% \item Will it provide a high compression ratio?
% \item How will it compare to existing solutions?
% learned compression---automatic identification of patterns in the data \& generation of compression trees: multiple pattern detectors, a cost model and compression learning algorithms
\fi

\noindent
Besides improved compression ratios, \textit{whitebox compression} in itself is an important contribution to the database research field as a new compression model. The transparent representation of logical columns as functions of physical columns through operator expressions simplifies the compression layer of database systems. It allows implicit recursive compression through an unlimited number of methods, while handling exceptions in a generic way. On top of this, \textit{whitebox compression} has the potential of improving query processing times by exposing the data representation to the query execution engine, allowing predicate push-down and lazy evaluation of the compression tree. This model can be implemented either as a stand-alone system with \textit{whitebox} versions of the existing compression methods or as an intermediate representation layer which creates compression opportunities for the optimized blackbox schemes.

\iffalse
% - real data is redundant and represented in inefficient ways. It is already suit-
% able for compression with existing lightweight methods, but it has a considerable untapped
% compression potential that could be exploited if the data had a different representation.
% - can be used to enhance existing systems as an intermediate layer before the optimized leaf compression methods
% - stand alone system with whitebox versions of existing lightweight techniques
% - creates opportunities for better compression with existing techniques
% - estimator wb leaf methods are not optimized, but even so wb compression brings an advantage; if optimized, could outperform the wb+vw combination; even so, wb can be used to enhance existing systems as an intermediate layer before the optimized leaf compression methods
% - even so, we showed that \textit{whitebox compression} can be used to enhance existing systems like VectorWise
% - creates opportunities for better compression with existing techniques
% - These results show the high degree of redundancy present in real data---and that we can squeeze this redundancy out of the data with a proper exception handling mechanism.
% - From our experiments we concluded that data can be represented and compressed in many ways. However, the learning algorithm needs to choose a single data representation, ideally the one that gives the smallest physical size
% - many logical columns, mostly varchar, are transformed into fewer physical columns, mostly numeric, at the expense of many exception columns, very sparse. ALTERNATIVE: The take-away from this analysis is that we can represent many logical columns---mostly \verb|VARCHAR|---as functions of fewer physical columns---mostly numeric---at the expense of many exception columns---mostly nulls---and achieve high compression ratios, all of this automatically learned.
\fi

\section{Future work}

For this thesis we explored and demonstrated the potential of \textit{whitebox compression} through a basic implementation. There is room for improvement and further development in all the components: pattern detectors, pattern selectors, learning algorithms, estimators. However, even with this initial exploratory approach we obtained good results, which only encourages future work in this direction, towards a highly optimized \textit{whitebox compression} model implemented in real systems.

Firstly, the Public BI benchmark deserves a more thorough characterisation, as it is a representative benchmark for database systems. In terms of data, an interesting result would be the distribution types of the numeric columns: skweness and kurtosis coefficients plotted on a Cullen and Frey graph. Moreover, outlier characterisation and the range and domain of values might also prove useful. Special attention should be directed towards the queries, to understand real use-cases in analytical systems.

There are compression opportunities in some datasets that are not yet covered by the current compression learning process, but could be easily exploited with a few improvements: 1) support for hexadecimal numbers in the \nameref{subsec:pd:numericstrings} pattern detector (hex-formatted columns in RealEstate* datasets); 2) a dedicated pattern selector for \nameref{subsec:pd:charsetsplit} which takes into account the number of characters that are not in the default charset; 3) padding whitespace detection and isolation on separate columns through a split instance to enable future compression with \nameref{subsec:pd:dict}; 4) column correlation support for continuous and discrete variables to leverage mathematical dependencies between numeric columns; 5) a different column split pattern detector based on frequencies of n-grams in string columns---approach and preliminary results are presented in Appendix~\ref{appendix:ngramanalysis}.

\textit{Whitebox compression} has potential for achieving fast query execution. However, this potential needs to be properly evaluated. Most importantly, we need efficient compression and decompression implementations. So far we implemented unoptimized versions of these procedures to evaluate the compression ratios and validate the correctness of our implementation by reconstructing the original data. An important step towards improved execution time is to use a different cost model for the learning algorithms---one that also takes into account the complexity of the compression tree (e.g. in terms of depth and branching factor). Going further, we can design a compression tree optimization process with the purpose of reducing its complexity (similar to query plan optimization). Furthermore, we should answer our 5th research question mentioned in the introduction: Can we achieve compressed execution with the \textit{whitebox compression} model? We need to study predicate push-down opportunities from the perspective of both data and queries, with the hope that we can (partially) skip decompression and operate directly on compressed data.

Finally, a machine learning approach for pattern detection and solving the compression learning optimization problem might lead to interesting results, provided that we can extract relevant features from the datasets and properly define our problem so that it fits the machine learning models.

\iffalse
% - there is a lot more to improve, we just tested and showed the potential of whitebox compression through a basic implementation; all components: pattern detectors, pattern selectors, learning algorithms, estimators can be improved (there is a lot of room for improvement); even with this initial exploratory approach we obtained good results (insert result here), which only encourages future work on this topic; 
% towards a highly optimized compression model and implemented in real systems, either as an intermediate representation layer or as a stand-alone system/storage/compression (and query execution) layer
% - (estimator) A more thorough experiment and analysis needs to be performed in order to properly evaluate the performance of a stand-alone \textit{whitebox} system in practice

*** pbib ***
% - characterisation of pbib: numeric columns skewness and kurtosis and plot them on a todo graph in order to find their distributions and characterisitcs (outlisers, skew, etc). comparison with synthetic benchmarks. characterisation of the queries

*** pd & learning ***
% - charsetsplit pattern selector; Score: f( number of chars that are not in the default charset “?”, coverage)
% - extendend suport for numeric strings (e.g. hex numbers---Hex formatted columns in [?] RealEstate*)
% - padding whitespace (e.g. isolate it on a separate column through a split instance to enable future compression with dictionary)
% - Other correlation types: continuous and discrete variables; ordinal categorical variables
% - ngram freq split

% - different charsetsplit instances with differente charsets, improved and dedicated pattern selection \& different priority configuration => possible better results

*** other ***
% - Different cost model \& learning algorithm; Optimized for both size and fast query execution; complexity of the (de)compression tree (depth, average branching factor, etc.)
% - Compression tree optimization
% - Efficient (de)compression implementation
% - Compressed execution
% - [?] Machine Learning approach
\fi

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------